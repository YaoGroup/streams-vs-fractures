{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75dfd8de-a66b-4c1f-a1bf-dac29bfa0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ecb9f0-ab48-463e-bdb4-6cf749e97720",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# IMPORTS\n",
    "###############################\n",
    "\n",
    "# torch things\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Sampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "# netCDF4 things\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset as ncDataset\n",
    "\n",
    "# normal python things\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#hyperparameter things\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d37aea6d-8187-4d0d-8e1d-d243c462eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "hyperparameters set\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 4, 294, 294]              40\n",
      "         MaxPool2d-2          [-1, 4, 147, 147]               0\n",
      "            Conv2d-3          [-1, 8, 147, 147]             296\n",
      "         MaxPool2d-4            [-1, 8, 73, 73]               0\n",
      "            Conv2d-5           [-1, 16, 73, 73]           1,168\n",
      "         MaxPool2d-6           [-1, 16, 36, 36]               0\n",
      "            Conv2d-7           [-1, 32, 36, 36]           4,640\n",
      "         MaxPool2d-8           [-1, 32, 18, 18]               0\n",
      "            Conv2d-9           [-1, 64, 18, 18]          18,496\n",
      "        MaxPool2d-10             [-1, 64, 9, 9]               0\n",
      "           Conv2d-11            [-1, 128, 9, 9]          73,856\n",
      "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
      "           Linear-13                    [-1, 1]           2,049\n",
      "================================================================\n",
      "Total params: 100,545\n",
      "Trainable params: 100,545\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.33\n",
      "Forward/backward pass size (MB): 6.44\n",
      "Params size (MB): 0.38\n",
      "Estimated Total Size (MB): 7.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# CNN ARCHITECTURE\n",
    "###############################\n",
    "\n",
    "# Create simple CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_features):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels=4, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # same convolution, meaning that the output size will be the same dimensions as the input (294x294) here\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # same convolution, meaning that the output size will be the same dimensions as the input (294x294) here\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.conv6 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.fc1 = nn.Linear(128*4*4, out_features) # size should be (out_channels from previous layer)*(input_size/2^num_layers) because the max pooling reduces dimension by 2 fold in each layer\n",
    "        # self.softplus = nn.Softplus()  # Define Softplus as a layer\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0],-1) # flattens to be able to put into the fully-connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = torch.abs(x)  # Ensure no negative values in the output (do EITHER this or softplus, not both)\n",
    "        # x = self.softplus(x) # Ensure no negative values in output (do EITHER this or abs, not both)\n",
    "\n",
    "        return x\n",
    "    \n",
    "###############################\n",
    "# SET GPU AND HYPERPARAMETERS\n",
    "###############################\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Hyperparameters\n",
    "in_channels = 1\n",
    "out_features = 1\n",
    "input_size = (1,294,294) #[C, H, W], the np arrays of images are [H, W], and the transform function converts to [H, W, C] \n",
    "\n",
    "# network structure\n",
    "# in_channels = [\n",
    "\n",
    "print(\"hyperparameters set\")\n",
    "\n",
    "###############################\n",
    "# INITIALIZE NETWORK\n",
    "###############################\n",
    "\n",
    "# Initialize network\n",
    "model = CNN(in_channels, out_features).to(device)\n",
    "\n",
    "# Print the model summary\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b35812c-bff2-4f75-bb37-9057c9261310",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# TRANSFORMATION DEFINITIONS\n",
    "###############################\n",
    "\n",
    "# define the desired transformations to the images and the labels (transform, transform_label)\n",
    "\n",
    "# transform: transforms images from numpy array of shape [H, W] to tensors of shape [C, H, W] in range [0, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Converts images to PyTorch tensors and scales to [0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "# transform_label: transforms the labels to a more normal distribution\n",
    "def transform_label_function(label, facs=3):\n",
    "    # Ensure label is a tensor\n",
    "    label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "    # Apply the transformation\n",
    "    transformed_label = (torch.log10(label_tensor + 1 * 10 ** -facs) + facs) / (facs - 1)\n",
    "    return transformed_label\n",
    "\n",
    "\n",
    "# inverse transform (to get back to original fracture density after the training)\n",
    "# be careful to make sure this is the inverse function to the transform function (manual)\n",
    "def inverse_transform_label_function(transformed_label, facs=3):\n",
    "    inverse_transformed_label = 10**((facs-1)*transformed_label - facs) - 10**(-facs)\n",
    "    return inverse_transformed_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e001d9-2035-4d9d-bcac-25e25816168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# DATASET CLASS CREATION FROM .nc FILES\n",
    "###############################\n",
    "\n",
    "# create custom dataset class from the .nc files\n",
    "class NCDataset(Dataset):\n",
    "    def __init__(self, nc_file_path, transform=None, transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nc_file_path (string): Path to the .nc file\n",
    "            transform (callable, optional): optional transform to apply to images\n",
    "            transform_label (callable, optional): optional transform to apply to the labels\n",
    "        \"\"\"\n",
    "        # load the .nc file\n",
    "        self.nc_data = ncDataset(nc_file_path, 'r')\n",
    "        \n",
    "        # access the images (image_data) and labels (fracture_density) from the .nc file\n",
    "        self.image_data = self.nc_data.variables['image_data']\n",
    "        self.mask_data = self.nc_data.variables['label_data']\n",
    "        self.fracture_density = self.nc_data.variables['fracture_density']\n",
    "        \n",
    "        # image and label transforms (transform, transform_label)\n",
    "        self.transform = transform\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "        # load whole array into memory (could also convert to float32 here if that is not already done in preprocessing)\n",
    "        self.image_data = self.image_data[:,:,:].astype(np.float32)\n",
    "        self.mask_data = self.mask_data[:,:,:].astype(np.float32)\n",
    "        self.fracture_density = self.fracture_density[:].astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.image_data.shape[2]  # number of images is the third dimension [2]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Extract a single image and its corresponding label\n",
    "        image = self.image_data[:, :, idx]\n",
    "        mask = self.mask_data[:, :, idx]\n",
    "        label = self.fracture_density[idx]\n",
    "        \n",
    "        # Apply image transformation if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # apply label transformation, if any\n",
    "        if self.transform_label:\n",
    "            label = self.transform_label(label)    \n",
    "            \n",
    "        # Convert to PyTorch tensors (might be repetitive)\n",
    "        #image = torch.tensor(image, dtype=torch.float32)\n",
    "        #label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # return\n",
    "        return image, label, mask\n",
    "   \n",
    "    def __reduce__(self):\n",
    "        return (self.__class__, (self.nc_file_path, self.transform, self.transform_label))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8a534-64d7-4bb2-892b-14a0b193a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# SET THE DATALOADERS\n",
    "###############################\n",
    "\n",
    "# .nc file path\n",
    "nc_file_path = 'data/data/orig_nc_files/QB02_20120729152314.nc'\n",
    "nc_file_path_val = 'data/data/orig_nc_files/QB02_20120731155001.nc'\n",
    "nc_file_path_test = 'data/data/orig_nc_files/QB02_20120731155004.nc'\n",
    "# load dataset from .nc file using NCDataset\n",
    "# ds_train = NCDataset(nc_file_path, transform=transform, transform_label=lambda x: transform_label_function(x, facs=3))\n",
    "# ds_val = NCDataset(nc_file_path_val, transform=transform, transform_label=lambda x: transform_label_function(x, facs=3))\n",
    "ds_train = NCDataset(nc_file_path, transform=transform, transform_label=None)\n",
    "ds_val = NCDataset(nc_file_path_val, transform=transform, transform_label=None)\n",
    "ds_test =  NCDataset(nc_file_path_test, transform=transform, transform_label=None)\n",
    "\n",
    "# set the sampler based on samples <= 0.05 (or some other value)\n",
    "weights = torch.ones(len(ds_train), dtype=torch.float)  # Start with all weights equal to 1\n",
    "zero_label_weight = 0.2  # Weight for zero-labeled samples, adjust as needed\n",
    "labels_train = np.array(nc.Dataset(nc_file_path, 'r').variables['fracture_density']).astype(np.float32) \n",
    "weights[labels_train == 0] = zero_label_weight  # Assign lower weight to zero-labeled samples\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=len(ds_train), replacement=True)\n",
    "\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, sampler=None, shuffle=True, num_workers=8)\n",
    "\n",
    "print(f'DataLoaders for train, val, and test sets have been set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b0adcd0-6199-417c-8c49-abf5d0679ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "plt.figure(5, 10)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ds_train['image_data'][:,:,idx])\n",
    "plt.title('Real Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ds_train['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "386c0cb5-6689-4aec-a074-54653634c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# GRID-SEARCH CV\n",
    "###############################\n",
    "\n",
    "model = NeuralNetClassifier(\n",
    "    CNN,\n",
    "    criterion = nn.MSELoss,\n",
    "    optimizer = optim.Adam,\n",
    "    module__in_channels = 1,\n",
    "    module__out_features = 1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'  # Specify device\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'max_epochs': [10, 50, 100],\n",
    "    'lr': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf0208d2-f3d9-45a6-a531-aadab21a5f09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 27 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n27 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/classifier.py\", line 165, in fit\n    return super(NeuralNetClassifier, self).fit(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1319, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1278, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1176, in fit_loop\n    dataset_train, dataset_valid = self.get_split_datasets(\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1771, in get_split_datasets\n    return self.train_split(dataset, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/dataset.py\", line 314, in __call__\n    raise bad_y_error\nValueError: Stratified CV requires explicitly passing a suitable y.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfracture_density\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 875\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 27 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n27 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/classifier.py\", line 165, in fit\n    return super(NeuralNetClassifier, self).fit(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1319, in fit\n    self.partial_fit(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1278, in partial_fit\n    self.fit_loop(X, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1176, in fit_loop\n    dataset_train, dataset_valid = self.get_split_datasets(\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/net.py\", line 1771, in get_split_datasets\n    return self.train_split(dataset, y, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/skorch/dataset.py\", line 314, in __call__\n    raise bad_y_error\nValueError: Stratified CV requires explicitly passing a suitable y.\n"
     ]
    }
   ],
   "source": [
    "grid.fit(ds_train.image_data.transpose(2,0,1), ds_train.fracture_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed617b-fbbc-495b-a2ae-c39219d357a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SEARCH COMPLETE')\n",
    "    print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f22a17-9fdf-4ffb-aa5b-e78176488a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# TRAINING LOOP FUNCTION\n",
    "###############################\n",
    "\n",
    "def training_loop(config):\n",
    "    learning_rate = config['lr']\n",
    "    batch_size = config['batch_size']\n",
    "    num_epochs = config['epochs']\n",
    "    \n",
    "    dataloader_train = DataLoader(config['train'], batch_size=batch_size, sampler=None, shuffle=True, num_workers=4)\n",
    "    dataloader_val = DataLoader(config['test'], batch_size=batch_size, sampler=None, num_workers=4)\n",
    "    #dataloader_test = DataLoader(ds_test, batch_size=batch_size, sampler=None, num_workers=4)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # List to store loss values\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    best_loss_val = float('inf')  # Initialize with a high value\n",
    "    best_loss_train = float('inf')  # Initialize with a high value\n",
    "\n",
    "    # beginning of training print statement:\n",
    "    print(f'train size[{len(ds_train)}]; val size[{len(ds_val)}]; batch size[{batch_size}]; batches per epoch[{len(dataloader_train)}]')\n",
    "\n",
    "    # Now use the DataLoader in the training loop\n",
    "    start_time_trainloop = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"\\r Starting epoch {epoch+1}\")\n",
    "\n",
    "        # start epoch timer\n",
    "        start_time_epoch = time.time() \n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader_train):\n",
    "            # print(f\"\\r Processing batch {batch_idx+1}\")\n",
    "\n",
    "            # put data to cuda if possible\n",
    "            images = images.to(device=device) # this moves the data tensor to the device which will carry out the computation\n",
    "            labels = labels.to(device=device) # this moves the target tensor to the device which will carry out the computation\n",
    "\n",
    "            # forward pass\n",
    "            preds = torch.squeeze(model(images)) # this does the forward pass of the data through the model and computes the model predictions (aka 'scores') for each imput in the batch. scores represents the model prediction for the given input data\n",
    "            # preds = torch.squeeze(preds)  # squeezes dimensions from [64, 1] to [64]. Now preds has shape [64], which is the same shape as the labels \n",
    "            loss = criterion(preds, labels) # this calculates the loss between the model prediction (scores) and the true label value (targets)\n",
    "            # print(f'preds shape[{preds.shape}]')\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad() # sets all gradients to zero at the beginning of each batch, so doesn't store the backprop calculations from previous forward props\n",
    "            loss.backward() # computes the gradients via backprop\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step() # updates the weights depending on the gradients computed in loss.backward\n",
    "\n",
    "            # Record the training loss for each iteration\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # record the training loss for each epoch\n",
    "        losses_train.append(loss.item())\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Validation phase (get validation loss and collect labels and predictions)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for images, labels in dataloader_val:\n",
    "                images = images.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                preds = torch.squeeze(model(images))\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                loss_val = criterion(preds, labels)\n",
    "                \n",
    "        losses_val.append(loss_val.item())\n",
    "        validation_accuracy = correct / total\n",
    "\n",
    "        # Log the validation accuracy\n",
    "        tune.report(loss=loss_val.item(), accuracy=validation_accuracy, epoch=epoch)\n",
    "\n",
    "\n",
    "        # save loss at the global minimum of the validation loss (loss_val)\n",
    "        if loss_val < best_loss_val:\n",
    "            # Update the best loss value\n",
    "            best_loss_val = loss_val\n",
    "            # Save the best model parameters\n",
    "            bestmodel_val_wts = copy.deepcopy(model.state_dict())\n",
    "            # Optionally, save to a file\n",
    "            torch.save(bestmodel_val_wts, 'bestmodel_wts_val'+num_epochs+'_'+learning_rate+'_'+batch_size+'.pt')\n",
    "\n",
    "        # save loss at the global minimum of the training loss (loss_train)\n",
    "        if loss < best_loss_train:\n",
    "            # Update the best loss value\n",
    "            best_loss_train = loss\n",
    "            # Save the best model parameters\n",
    "            bestmodel_train_wts = copy.deepcopy(model.state_dict())\n",
    "            # Optionally, save to a file\n",
    "            torch.save(bestmodel_train_wts, 'bestmodel_wts_train'+num_epochs+'_'+learning_rate+'_'+batch_size+'.pt')\n",
    "\n",
    "        # end epoch timer\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        elapsed_time_training = (end_time_epoch-start_time_trainloop)/60 #[minutes]\n",
    "\n",
    "        # print end-of-epoch statement\n",
    "        sys.stdout.write(f'\\r epoch[{epoch+1}/{num_epochs}]; train loss[{loss.item():.8f}]; val loss[{loss_val.item():.8f}]; elapsed time[{elapsed_time_training:.4f} mins]')\n",
    "    \n",
    "    return validation_accuracy\n",
    "\n",
    "    # NOTE: 'targets' means the ground-truth labels, 'scores' or 'preds' are model predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efb07e-1ea1-46f0-a68d-f85dbf3acb78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Dataset is not picklable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[1;32m     11\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     max_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     14\u001b[0m     grace_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,)\n\u001b[1;32m     17\u001b[0m reporter \u001b[38;5;241m=\u001b[39m CLIReporter(\n\u001b[1;32m     18\u001b[0m     parameter_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     metric_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m],)\n\u001b[0;32m---> 21\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTuneConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m# Start hyperparameter search\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03manalysis = tune.run(training_loop,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    progress_reporter=reporter)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/tune/tuner.py:179\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, trainable, param_space, tune_config, run_config, _tuner_kwargs, _tuner_internal, _entrypoint)\u001b[0m\n\u001b[1;32m    177\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(_SELF, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_ray_client:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_tuner \u001b[38;5;241m=\u001b[39m \u001b[43mTunerInternal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_tuner \u001b[38;5;241m=\u001b[39m _force_on_current_node(\n\u001b[1;32m    182\u001b[0m         ray\u001b[38;5;241m.\u001b[39mremote(num_cpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(TunerInternal)\n\u001b[1;32m    183\u001b[0m     )\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:134\u001b[0m, in \u001b[0;36mTunerInternal.__init__\u001b[0;34m(self, restore_path, storage_filesystem, resume_config, trainable, param_space, tune_config, run_config, _tuner_kwargs, _entrypoint)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m trainable\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverted_trainable\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_trainable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_trainable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_space \u001b[38;5;241m=\u001b[39m param_space\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resume_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:208\u001b[0m, in \u001b[0;36mTunerInternal._validate_trainable\u001b[0;34m(self, trainable, required_trainable_name)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determines whether or not the trainable is valid.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mThis includes checks on the serializability of the trainable, as well\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        is not serializable.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     sio \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle_fast.py:88\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     87\u001b[0m     cp \u001b[38;5;241m=\u001b[39m CloudPickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle_fast.py:733\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2648\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__reduce__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Dataset is not picklable"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# RAY-TUNE\n",
    "###############################\n",
    "# config = {\n",
    "#     \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "#     \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "#     \"epochs\": tune.choice([20, 60, 100]),\n",
    "# }\n",
    "\n",
    "# # Initialize Ray Tune\n",
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=100,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2,)\n",
    "\n",
    "# reporter = CLIReporter(\n",
    "#     parameter_columns=[\"batch_size\", \"lr\", \"epochs\"],\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"],)\n",
    "\n",
    "# tuner = tune.Tuner(\n",
    "#     tune.with_resources(\n",
    "#         tune.with_parameters(training_loop),\n",
    "#         resources={\"cpu\": 2}\n",
    "#     ),\n",
    "#     tune_config=tune.TuneConfig(\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "#         scheduler=scheduler,\n",
    "#         num_samples=10,\n",
    "#     ),\n",
    "#     param_space=config\n",
    "# )\n",
    "# '''\n",
    "# # Start hyperparameter search\n",
    "# analysis = tune.run(training_loop,\n",
    "#     config=config,\n",
    "#     resources_per_trial={\"cpu\": 2},\n",
    "#     num_samples=10,\n",
    "#     scheduler=scheduler,\n",
    "#     progress_reporter=reporter)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f5ff0-9ce6-4dd1-a20d-b8761000419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = analysis.get_best_trial(metric=\"accuracy\", mode=\"max\")\n",
    "best_hyperparameters = best_trial.config\n",
    "best_accuracy = best_trial.last_result[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43859d36-555d-4822-a760-2bf4b78d6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# AFTER THE TRAINING LOOP CONCLUDES:\n",
    "# LOAD THE TRAINED MODEL\n",
    "########################################\n",
    "\n",
    "# After training is complete, you can load the best model weights back into your model\n",
    "model = CNN(in_channels, out_features)\n",
    "weights_path = 'bestmodel_wts_train.pt'\n",
    "model.load_state_dict(torch.load(weights_path)) # load the model state from the desired output file (either best validation model or best training model)\n",
    "model.to(device) # send model to GPU\n",
    "model.eval()  # Evaluate model mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753107d1-54e1-4ff4-8714-4430d6b3cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# AFTER THE TRAINING LOOP CONCLUDES:\n",
    "# GET LABELS AND PREDICTIONS FOR TRAIN AND VAL SETS FROM THE TRAINED MODEL\n",
    "########################################\n",
    "\n",
    "# get labels and predictions for both training set and validation set\n",
    "# training set:\n",
    "model.eval()  # Evaluate mode\n",
    "predictions_train = []\n",
    "actual_labels_train = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_train:  # Assuming you want to do this for the training set\n",
    "        images = images.to(device)\n",
    "        preds = model(images)\n",
    "        predictions_train.extend(preds.view(-1).tolist())  # Flatten and convert to list\n",
    "        actual_labels_train.extend(labels.tolist())\n",
    "# Ensure predictions and actual_labels are numpy arrays for easier manipulation\n",
    "predictions_train = np.array(predictions_train)\n",
    "actual_labels_train = np.array(actual_labels_train)\n",
    "\n",
    "print(f\"train set: predictions and labels acquired ({len(actual_labels_train)})\")\n",
    "    \n",
    "# validation set:\n",
    "model.eval()  # Evaluate mode\n",
    "predictions_val = []\n",
    "actual_labels_val = []\n",
    "with torch.no_grad():\n",
    "    for images_val, labels_val in dataloader_val:  # Assuming you want to do this for the training set\n",
    "        images_val = images_val.to(device)\n",
    "        preds_val = model(images_val)\n",
    "        predictions_val.extend(preds_val.view(-1).tolist())  # Flatten and convert to list\n",
    "        actual_labels_val.extend(labels_val.tolist())\n",
    "# Ensure predictions and actual_labels are numpy arrays for easier manipulation\n",
    "predictions_val = np.array(predictions_val)\n",
    "actual_labels_val = np.array(actual_labels_val)\n",
    "\n",
    "print(f\"validation set: predictions and labels acquired ({len(actual_labels_val)})\")\n",
    "\n",
    "# get the training and validation RMSE (transformed and un-transformed)\n",
    "rmse_train_transf = np.sqrt(np.mean((actual_labels_train - predictions_train) ** 2))\n",
    "rmse_val_transf = np.sqrt(np.mean((actual_labels_val - predictions_val) ** 2))\n",
    "# rmse_train_untransf = np.sqrt(np.mean((inverse_transform_label_function(actual_labels_train, facs=3) - inverse_transform_label_function(predictions_train, facs=3)) ** 2))\n",
    "# rmse_val_untransf = np.sqrt(np.mean((inverse_transform_label_function(actual_labels_val, facs=3) - inverse_transform_label_function(predictions_val, facs=3)) ** 2))\n",
    "\n",
    "# print(f'training set RMSE:\\n rmse_train_transf[{rmse_train_transf:.4f}]\\n rmse_val_transf[{rmse_val_transf:.4f}]\\nvalidation set RMSE: \\n rmse_train_untransf[{rmse_train_untransf:.4f}]\\n rmse_val_untransf[{rmse_val_untransf:.4f}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990e5f5-f05b-400d-87f4-2a537adaf76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# PLOTS\n",
    "###############################\n",
    "\n",
    "# plot of training and validation losses across epochs\n",
    "# plt.figure()\n",
    "# plt.plot(losses_train, c='black', label='training loss')\n",
    "# plt.plot(losses_val, c='red', label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.title(f'training and validation losses per epoch; {num_epochs} epochs; {elapsed_time_training:.3f} minutes')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.ylim(0,0.005)\n",
    "\n",
    "# plot of the predictions versus labels for training set and validation set (transformed labels)\n",
    "plt.figure()\n",
    "plt.scatter(actual_labels_train, predictions_train, c='black', s=12, label='train set')\n",
    "plt.scatter(actual_labels_val, predictions_val, c='red', s=7, label='val set')\n",
    "plt.plot(np.array(np.linspace(0,1,11)),np.array(np.linspace(0,1,11)),'b-')\n",
    "plt.title('predictions vs actual labels')\n",
    "plt.xlabel('actual (ground truth) labels')\n",
    "plt.ylabel('predictions (CNN preds)')\n",
    "plt.xlim([0, 0.5])\n",
    "plt.ylim([0, 0.5])\n",
    "plt.legend()\n",
    "\n",
    "# # plot of the predictions versus labels for training set and validation set (un-transformed labels)\n",
    "# plt.figure()\n",
    "# plt.scatter((actual_labels_train), (predictions_train), c='black', s=10, label='train set')\n",
    "# # plt.scatter(inverse_transform_label_function(actual_labels_val, facs=3), inverse_transform_label_function(predictions_val, facs=3), c='red', s=7, label='val set')\n",
    "# plt.title('predictions vs actual labels')\n",
    "# plt.xlabel('actual labels')\n",
    "# plt.ylabel('predictions')\n",
    "# plt.xlim([0, 0.4])\n",
    "# plt.ylim([0, 0.4])\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db717355-0c76-4b12-a350-a34d5077fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# PLOT TRAINING LOSS CURVE AT EACH ITERATION\n",
    "###############################\n",
    "\n",
    "# Calculate the total number of batches processed\n",
    "total_batches = len(dataloader_train) * num_epochs\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "plt\n",
    "plt.plot(losses, label='training Loss')\n",
    "\n",
    "# # Add vertical lines for each epoch\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Calculate the iteration index at the end of each epoch\n",
    "#     epoch_end_iter = len(data_loader) * (epoch + 1)\n",
    "#     plt.axvline(x=epoch_end_iter, color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylim(0,0.05)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('training loss as a function of iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d494ed-ab24-4d22-857a-8f21b03980d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# PREDICTING TEST SET\n",
    "###############################"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
